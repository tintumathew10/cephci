tests:
- test:
    abort-on-fail: true
    desc: Install software pre-requisites for cluster deployment.
    module: install_prereq.py
    name: setup pre-requisites
- test:
    abort-on-fail: true
    clusters:
      ceph-pri:
        config:
          verify_cluster_health: true
          steps:
          - config:
              command: bootstrap
              service: cephadm
              args:
                registry-url: registry.redhat.io
                mon-ip: node1
                orphan-initial-daemons: true
                initial-dashboard-password: admin@123
                dashboard-password-noupdate: true
                skip-monitoring-stack: true
          - config:
              command: add_hosts
              service: host
              args:
                attach_ip_address: true
                labels: apply-all-labels
          - config:
              command: apply
              service: mgr
              args:
                placement:
                  label: mgr
          - config:
              command: apply
              service: mon
              args:
                placement:
                  label: mon
          - config:
              command: apply
              service: osd
              args:
                all-available-devices: true
          - config:
              command: apply
              service: rgw
              pos_args:
              - shared.pri
              args:
                placement:
                  nodes:
                  - node5
      ceph-sec:
        config:
          verify_cluster_health: true
          steps:
          - config:
              command: bootstrap
              service: cephadm
              args:
                registry-url: registry.redhat.io
                mon-ip: node1
                orphan-initial-daemons: true
                initial-dashboard-password: admin@123
                dashboard-password-noupdate: true
                skip-monitoring-stack: true
          - config:
              command: add_hosts
              service: host
              args:
                attach_ip_address: true
                labels: apply-all-labels
          - config:
              command: apply
              service: mgr
              args:
                placement:
                  label: mgr
          - config:
              command: apply
              service: mon
              args:
                placement:
                  label: mon
          - config:
              command: apply
              service: osd
              args:
                all-available-devices: true
          - config:
              command: apply
              service: rgw
              pos_args:
              - shared.sec
              args:
                placement:
                  nodes:
                  - node5
    desc: RHCS cluster deployment using cephadm.
    polarion-id: CEPH-83575222
    destroy-cluster: false
    module: test_cephadm.py
    name: deploy cluster
    testrail-id: 141109
- test:
    clusters:
      ceph-pri:
        config:
          verify_cluster_health: true
          steps:
          - config:
              command: apply_spec
              service: orch
              validate-spec-services: true
              specs:
              - service_type: prometheus
                placement:
                  count: 1
                  nodes:
                  - node1
              - service_type: grafana
                placement:
                  nodes:
                  - node1
              - service_type: alertmanager
                placement:
                  count: 1
              - service_type: node-exporter
                placement:
                  host_pattern: '*'
              - service_type: crash
                placement:
                  host_pattern: '*'
      ceph-sec:
        config:
          verify_cluster_health: true
          steps:
          - config:
              command: apply_spec
              service: orch
              validate-spec-services: true
              specs:
              - service_type: prometheus
                placement:
                  count: 1
                  nodes:
                  - node1
              - service_type: grafana
                placement:
                  nodes:
                  - node1
              - service_type: alertmanager
                placement:
                  count: 1
              - service_type: node-exporter
                placement:
                  host_pattern: '*'
              - service_type: crash
                placement:
                  host_pattern: '*'
    name: Monitoring Services deployment
    desc: Add monitoring services using spec file.
    module: test_cephadm.py
    polarion-id: CEPH-83574727
    testrail-id: 141349
- test:
    abort-on-fail: true
    clusters:
      ceph-pri:
        config:
          command: add
          id: client.1
          node: node6
          install_packages:
          - ceph-common
          copy_admin_keyring: true
      ceph-sec:
        config:
          command: add
          id: client.1
          node: node6
          install_packages:
          - ceph-common
          copy_admin_keyring: true
    desc: Configure the RGW client system
    polarion-id: CEPH-83573758
    destroy-cluster: false
    module: test_client.py
    name: configure client
    testrail-id: 141520
- test:
    abort-on-fail: true
    clusters:
      ceph-pri:
        config:
          haproxy_clients:
          - node6
          rgw_endpoints:
          - node5:80
      ceph-sec:
        config:
          haproxy_clients:
          - node6
          rgw_endpoints:
          - node5:80
    desc: Configure HAproxy
    module: haproxy.py
    name: Configure HAproxy
- test:
    abort-on-fail: true
    clusters:
      ceph-pri:
        config:
          cephadm: true
          commands:
          - radosgw-admin realm create --rgw-realm india --default
          - radosgw-admin zonegroup create --rgw-realm india --rgw-zonegroup shared
            --endpoints http://{node_ip:node5}:80 --master --default
          - radosgw-admin zone create --rgw-realm india --rgw-zonegroup shared --rgw-zone
            primary --endpoints http://{node_ip:node5}:80 --master --default
          - radosgw-admin period update --rgw-realm india --commit
          - radosgw-admin user create --uid=repuser --display_name='Replication user'
            --access-key 21e86bce636c3aa0 --secret cf764951f1fdde5d --rgw-realm india
            --system
          - radosgw-admin zone modify --rgw-realm india --rgw-zonegroup shared --rgw-zone
            primary --access-key 21e86bce636c3aa0 --secret cf764951f1fdde5d
          - radosgw-admin period update --rgw-realm india --commit
          - ceph config set client.rgw.{daemon_id:shared.pri} rgw_realm india
          - ceph config set client.rgw.{daemon_id:shared.pri} rgw_zonegroup shared
          - ceph config set client.rgw.{daemon_id:shared.pri} rgw_zone primary
          - ceph orch restart {service_name:shared.pri}
      ceph-sec:
        config:
          cephadm: true
          commands:
          - sleep 120
          - radosgw-admin realm pull --rgw-realm india --url http://{node_ip:ceph-pri#node5}:80
            --access-key 21e86bce636c3aa0 --secret cf764951f1fdde5d --default
          - radosgw-admin period pull --url http://{node_ip:ceph-pri#node5}:80 --access-key
            21e86bce636c3aa0 --secret cf764951f1fdde5d
          - radosgw-admin zone create --rgw-realm india --rgw-zonegroup shared --rgw-zone
            secondary --endpoints http://{node_ip:node5}:80 --access-key 21e86bce636c3aa0
            --secret cf764951f1fdde5d
          - radosgw-admin period update --rgw-realm india --commit
          - ceph config set client.rgw.{daemon_id:shared.sec} rgw_realm india
          - ceph config set client.rgw.{daemon_id:shared.sec} rgw_zonegroup shared
          - ceph config set client.rgw.{daemon_id:shared.sec} rgw_zone secondary
          - ceph orch restart {service_name:shared.sec}
    desc: Setting up RGW multisite replication environment
    module: exec.py
    name: setup multisite
    polarion-id: CEPH-10362
    testrail-id: 141074
- test:
    abort-on-fail: true
    clusters:
      ceph-pri:
        config:
          cephadm: true
          commands:
          - radosgw-admin sync status
          - ceph -s
          - radosgw-admin realm list
          - radosgw-admin zonegroup list
          - radosgw-admin zone list
    desc: Retrieve the configured environment details
    polarion-id: CEPH-83575227
    module: exec.py
    name: get shared realm info on primary
    testrail-id: 141110
- test:
    abort-on-fail: true
    clusters:
      ceph-sec:
        config:
          cephadm: true
          commands:
          - radosgw-admin sync status
          - ceph -s
          - radosgw-admin realm list
          - radosgw-admin zonegroup list
          - radosgw-admin zone list
    desc: Retrieve the configured environment details
    polarion-id: CEPH-83575227
    module: exec.py
    name: get shared realm info on secondary
    testrail-id: 141110
- test:
    clusters:
      ceph-pri:
        config:
          set-env: true
          script-name: user_create.py
          config-file-name: non_tenanted_user.yaml
          copy-user-info-to-site: ceph-sec
    desc: create non-tenanted user
    polarion-id: CEPH-83575199
    module: sanity_rgw_multisite.py
    name: create non-tenanted user
    testrail-id: 140234
- test:
    name: delete versioned objects on secondary
    desc: test_versioning_objects_delete on secondary
    polarion-id: CEPH-14262
    module: sanity_rgw_multisite.py
    clusters:
      ceph-sec:
        config:
          script-name: test_versioning_with_objects.py
          config-file-name: test_versioning_objects_delete.yaml
    testrail-id: 141177
- test:
    name: delete bucket policy on secondary
    desc: test_bucket_policy_delete.yaml on secondary
    polarion-id: CEPH-11213
    module: sanity_rgw_multisite.py
    clusters:
      ceph-pri:
        config:
          script-name: test_bucket_policy_ops.py
          config-file-name: test_bucket_policy_delete.yaml
          verify-io-on-site:
          - ceph-pri
    testrail-id: 140020
- test:
    name: download objects on secondary
    desc: test_Mbuckets_with_Nobjects_download on secondary
    polarion-id: CEPH-14237
    module: sanity_rgw_multisite.py
    clusters:
      ceph-sec:
        config:
          script-name: test_Mbuckets_with_Nobjects.py
          verify-io-on-site:
          - ceph-pri
          config-file-name: test_Mbuckets_with_Nobjects_download.yaml
    testrail-id: 141069
- test:
    name: multipart upload on primary
    desc: test_Mbuckets_with_Nobjects_multipart on primary
    polarion-id: CEPH-14265
    module: sanity_rgw_multisite.py
    clusters:
      ceph-pri:
        config:
          script-name: test_Mbuckets_with_Nobjects.py
          verify-io-on-site:
          - ceph-sec
          config-file-name: test_Mbuckets_with_Nobjects_multipart.yaml
    testrail-id: 141195
- test:
    name: rgw account with multisite,tenancy,reusability
    desc: rgw account with multisite,tenancy,reusability
    polarion-id: CEPH-83591673
    module: sanity_rgw_multisite.py
    clusters:
      ceph-pri:
        config:
          script-name: test_bucket_lifecycle_object_expiration_transition.py
          config-file-name: test_lc_date_rgw_accounts.yaml
    testrail-id: 141146
